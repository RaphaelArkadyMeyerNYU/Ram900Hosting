<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=stylesheet  href="/css/theorem.css"> <link rel=stylesheet  href="/css/algorithm.css"> <link rel=icon  href="/assets/favicon.ico"> <title>SIAM-NNP: The Matrix-Vector Complexity of Linear Algebra</title> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <li><a href="/assets/raphael_a_meyer_cv.pdf">CV</a> <li><a href="/hutchplusplus" style="padding: 10px 8px">Hutch++</a> <li><a href="/siam-nnp-minisymposium-2023" style="padding: 10px 0px">SIAM-NNP</a> </ul> </div> <div id=main > <div class=franklin-content > <style> .franklin-content{ width:700px } </style> <h1 id=siam-nnp_minisymposium_on_the_matrix-vector_complexity_of_linear_algebra ><a href="#siam-nnp_minisymposium_on_the_matrix-vector_complexity_of_linear_algebra" class=header-anchor >SIAM-NNP Minisymposium on The Matrix-Vector Complexity of Linear Algebra</a></h1> <p>At the <a href="https://sites.google.com/view/siam-nynjpa/annual-meeting">SIAM-NNP conference</a>, I am organizing a minisymposium on matrix-vector complexity. This is a small webpage to track the time and place of the talks, as well as the titles and abstracts of the talks.</p> <h2 id=when_and_where_is_the_event ><a href="#when_and_where_is_the_event" class=header-anchor >When and where is the event?</a></h2> <p>The talks are at NJIT on October 22nd, 2023. Specifically, they will run from 8:30am to 10:30am in the Central King Building, on either Floor 2 or Floor 3 &#40;exact location TBD&#41;.</p> <p>To attend, you have to register for the conference <a href="https://sites.google.com/view/siam-nynjpa/annual-meeting/registration">here</a>. It&#39;s free for students and postdocs&#33;</p> <h2 id=schedule ><a href="#schedule" class=header-anchor >Schedule</a></h2> <style> .franklin-content .fndef tr, td { padding: 1.5px 5px; font-size: 16px; } .franklin-content th { font-size: 17px; } .franklin-content table td:nth-child(2) { width: 130px; } .franklin-content table tr:nth-child(2), .franklin-content table tr:nth-child(4), .franklin-content table tr:nth-child(6), .franklin-content table tr:nth-child(8), .franklin-content table tr:nth-child(10), .franklin-content table tr:nth-child(12), .franklin-content table tr:nth-child(14), .franklin-content table tr:nth-child(16) { background: #f3f3ec; } .franklin-content table tr:nth-child(2) a, .franklin-content table tr:nth-child(4) a, .franklin-content table tr:nth-child(6) a, .franklin-content table tr:nth-child(8) a, .franklin-content table tr:nth-child(10) a, .franklin-content table tr:nth-child(12) a, .franklin-content table tr:nth-child(14) a, .franklin-content table tr:nth-child(16) a { text-shadow: 0.03em 0 #f3f3ec, -0.03em 0 #f3f3ec, 0 0.03em #f3f3ec, 0 -0.03em #f3f3ec, 0.06em 0 #f3f3ec, -0.06em 0 #f3f3ec, 0.09em 0 #f3f3ec, -0.09em 0 #f3f3ec, 0.12em 0 #f3f3ec, -0.12em 0 #f3f3ec, 0.15em 0 #f3f3ec, -0.15em 0 #f3f3ec; } </style> <table><tr><th align=left >Time<th align=left >Presenter<th align=left >University<th align=left >Talk Title &#40;click to jump to the abstract&#41;<tr><td align=left >8:30am<td align=left ><a href="https://sites.google.com/view/shyamnarayanan/home">Shyam Narayanan</a><td align=left >MIT<td align=left ><a href="#new_lower_bounds_for_matrix-vector_algorithms">New Lower Bounds for Matrix-Vector Algorithms</a><tr><td align=left >9:00am<td align=left ><a href="https://e.math.cornell.edu/people/halikias/">Diana Halikias</a><td align=left >Cornell<td align=left ><a href="#data-efficient_matrix_recovery_and_pde_learnings">Data-efficient matrix recovery and PDE learning</a><tr><td align=left >9:30am<td align=left ><a href="https://wswartworth.github.io/">William Swartworth</a><td align=left >CMU<td align=left ><a href="#spectrum_approximation_via_non-adaptive_queries">Spectrum Approximation via non-adaptive Queries</a><tr><td align=left >10:00am<td align=left ><a href="https://chen.pw/">Tyler Chen</a><td align=left >NYU<td align=left ><a href="#peering_into_the_black_box_krylov-aware_stochastic_trace_estimation">Peering into the black box: Krylov-aware stochastic trace estimation</a><tr><td align=left >10:30am<td align=left ><a href="https://ram900.hosting.nyu.edu/">Raphael Meyer</a><td align=left >NYU<td align=left ><a href="#hutchinsons_estimator_is_bad_at_kronecker-trace-estimation">Hutchinson&#39;s Estimator is Bad at Kronecker-Trace-Estimation</a></table> <h2 id=abstracts_for_the_talks ><a href="#abstracts_for_the_talks" class=header-anchor >Abstracts for the Talks</a></h2> <h4 id=new_lower_bounds_for_matrix-vector_algorithms ><a href="#new_lower_bounds_for_matrix-vector_algorithms" class=header-anchor >New Lower Bounds for Matrix-Vector Algorithms</a></h4> <p>The Matrix-Vector model is a model of computation where, given an unknown matrix <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >M</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{ M}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68611em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.11424em;">M</span></span></span></span></span></span>, one attempts to learn properties of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >M</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{ M}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68611em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.11424em;">M</span></span></span></span></span></span> through an oracle that receives as input a vector <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold >v</mi></mrow><annotation encoding="application/x-tex">\mathbf{ v}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44444em;vertical-align:0em;"></span><span class=mord ><span class="mord mathbf" style="margin-right:0.01597em;">v</span></span></span></span></span> and outputs <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >M</mi><mi mathvariant=bold >v</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{ M}\mathbf{ v}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68611em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.11424em;">M</span></span></span><span class=mord ><span class="mord mathbf" style="margin-right:0.01597em;">v</span></span></span></span></span>. This model captures a wide range of algorithms used in optimization and numerical linear algebra. This model can also be viewed as a sublinear-query model, where one wishes to ask a sublinear number of queries to the oracle to learn properties of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >M</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{ M}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68611em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.11424em;">M</span></span></span></span></span></span>. In this talk, we demonstrate a new technique for proving lower bounds for the query complexity of various matrix-vector algorithms. The technique is based on a novel reduction that demonstrates that &quot;block Krylov&quot; algorithms are optimal for this problem. This allows us to prove lower bounds for two important questions:</p> <ol> <li><p>Log-concave sampling: we show that sampling from strongly log-concave and log-smooth distributions with condition number <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>κ</mi></mrow><annotation encoding="application/x-tex">\kappa</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">κ</span></span></span></span> requires <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent=true ><mi mathvariant=normal >Ω</mi><mo>~</mo></mover><mo stretchy=false >(</mo><mi>min</mi><mo>⁡</mo><mo stretchy=false >(</mo><msqrt><mi>κ</mi></msqrt><mi>log</mi><mo>⁡</mo><mi>d</mi><mo separator=true >,</mo><mi>d</mi><mo stretchy=false >)</mo><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">\tilde{\Omega}(\min(\sqrt{\kappa} \log d, d))</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.1701899999999998em;vertical-align:-0.25em;"></span><span class="mord accent"><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.9201899999999998em;"><span style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=mord ><span class=mord >Ω</span></span></span><span style="top:-3.6023300000000003em;"><span class=pstrut  style="height:3em;"></span><span class=accent-body  style="left:-0.25em;"><span class=mord >~</span></span></span></span></span></span></span><span class=mopen >(</span><span class=mop >min</span><span class=mopen >(</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8002800000000001em;"><span class=svg-align  style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=mord  style="padding-left:0.833em;"><span class="mord mathnormal">κ</span></span></span><span style="top:-2.76028em;"><span class=pstrut  style="height:3em;"></span><span class=hide-tail  style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z'/></svg></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.23972em;"><span></span></span></span></span></span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">d</span><span class=mpunct >,</span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">d</span><span class=mclose >)</span><span class=mclose >)</span></span></span></span> first-order queries. In fact, this bound even holds &#40;and is tight&#41; for the more restricted class of Gaussian distributions. Importantly, we provide the first lower bound for general log-concave distributions proving that the query complexity of general log-concave sampling cannot be dimension-free &#40;i.e., only depend on <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>κ</mi></mrow><annotation encoding="application/x-tex">\kappa</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">κ</span></span></span></span>&#41;.</p> <li><p>Low-rank approximation: we show that providing a <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy=false >(</mo><mn>1</mn><mo>+</mo><mi>ϵ</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">(1 + \epsilon)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class=mopen >(</span><span class=mord >1</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span><span class=mbin >+</span><span class=mspace  style="margin-right:0.2222222222222222em;"></span></span><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ϵ</span><span class=mclose >)</span></span></span></span>-approximate rank-1 approximation to <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >M</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{ M}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68611em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol" style="margin-right:0.11424em;">M</span></span></span></span></span></span> &#40;in spectral error&#41; requires <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >Ω</mi><mo stretchy=false >(</mo><mi>log</mi><mo>⁡</mo><mi>d</mi><mi mathvariant=normal >/</mi><msqrt><mi>ϵ</mi></msqrt><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">\Omega(\log d/\sqrt{\epsilon})</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.05028em;vertical-align:-0.25em;"></span><span class=mord >Ω</span><span class=mopen >(</span><span class=mop >lo<span style="margin-right:0.01389em;">g</span></span><span class=mspace  style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">d</span><span class=mord >/</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.8002800000000001em;"><span class=svg-align  style="top:-3em;"><span class=pstrut  style="height:3em;"></span><span class=mord  style="padding-left:0.833em;"><span class="mord mathnormal">ϵ</span></span></span><span style="top:-2.76028em;"><span class=pstrut  style="height:3em;"></span><span class=hide-tail  style="min-width:0.853em;height:1.08em;"><svg width='400em' height='1.08em' viewBox='0 0 400000 1080' preserveAspectRatio='xMinYMin slice'><path d='M95,702 c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14 c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54 c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10 s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429 c69,-144,104.5,-217.7,106.5,-221 l0 -0 c5.3,-9.3,12,-14,20,-14 H400000v40H845.2724 s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7 c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z M834 80h400000v40h-400000z'/></svg></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.23972em;"><span></span></span></span></span></span><span class=mclose >)</span></span></span></span> samples, which is optimal. For this question, no super-constant lower bound &#40;either in terms of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">ϵ</span></span></span></span> or in <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">d</span></span></span></span>&#41; was known.</p> </ol> <hr /> <h4 id=data-efficient_matrix_recovery_and_pde_learnings ><a href="#data-efficient_matrix_recovery_and_pde_learnings" class=header-anchor >Data-efficient matrix recovery and PDE learnings</a></h4> <p>Can one recover the entries of a matrix from only matrix-vector products? If so, how many are needed? I will present randomized algorithms to recover various structured matrices, as well as theoretical results which bound the query complexity of these structured families. Moreover, a continuous generalization of query complexity describes how many pairs of forcing terms and solutions are needed to uniquely identify a Green&#39;s function corresponding to the solution operator of an elliptic PDE. I will present a recent main result, which is a theoretical guarantee on the number of input-output pairs required in elliptic PDE learning problems with high probability. The proof of this result is constructive, and leverages insights from numerical linear algebra and PDE theory.</p> <hr /> <h4 id=spectrum_approximation_via_non-adaptive_queries ><a href="#spectrum_approximation_via_non-adaptive_queries" class=header-anchor >Spectrum Approximation via non-adaptive Queries</a></h4> <p>Variants of Power Method are useful for approximating the top eigenvalues of a matrix, but these methods rely heavily on adaptivity and typically only learn the top portion of the spectrum. How well can one do if the goal is to approximate the entire spectrum via non-adaptive matrix-vector queries? For a symmetric matrix <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >A</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{ A}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68611em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol">A</span></span></span></span></span></span>, this talk will show how to recover all eigenvalues of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >A</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{ A}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68611em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol">A</span></span></span></span></span></span> to within <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi><mi mathvariant=normal >∥</mi><mi mathvariant=bold-italic >A</mi><msub><mi mathvariant=normal >∥</mi><mi>F</mi></msub></mrow><annotation encoding="application/x-tex">\varepsilon\| \boldsymbol{ A}\|_F</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ε</span><span class=mord >∥</span><span class=mord ><span class=mord ><span class="mord boldsymbol">A</span></span></span><span class=mord ><span class=mord >∥</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">F</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> additive error using <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy=false >(</mo><mn>1</mn><mi mathvariant=normal >/</mi><msup><mi>ε</mi><mn>2</mn></msup><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">O(1/\varepsilon^2)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=mopen >(</span><span class=mord >1</span><span class=mord >/</span><span class=mord ><span class="mord mathnormal">ε</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span> matrix-vector queries. I will also discuss a lower bound showing that <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=normal >Ω</mi><mo stretchy=false >(</mo><mn>1</mn><mi mathvariant=normal >/</mi><msup><mi>ε</mi><mn>2</mn></msup><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">\Omega(1/\varepsilon^2)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1.064108em;vertical-align:-0.25em;"></span><span class=mord >Ω</span><span class=mopen >(</span><span class=mord >1</span><span class=mord >/</span><span class=mord ><span class="mord mathnormal">ε</span><span class=msupsub ><span class=vlist-t ><span class=vlist-r ><span class=vlist  style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class=mclose >)</span></span></span></span> matrix-vector queries are necessary for obtaining our guarantee, even if we allow adaptivity.</p> <hr /> <h4 id=peering_into_the_black_box_krylov-aware_stochastic_trace_estimation ><a href="#peering_into_the_black_box_krylov-aware_stochastic_trace_estimation" class=header-anchor >Peering into the black box: Krylov-aware stochastic trace estimation</a></h4> <p>Matrix-vector query models have gained increased attention, and in that case that the matrix of interest is a matrix function &#40;e.g. the matrix inverse or matrix exponential&#41;, it is common to use Krylov subspace methods as a black box for computing matrix-vector products. In this talk, we discuss how taking a look into the black-box allows further efficiencies to be obtained.</p> <hr /> <h4 id=hutchinsons_estimator_is_bad_at_kronecker-trace-estimation ><a href="#hutchinsons_estimator_is_bad_at_kronecker-trace-estimation" class=header-anchor >Hutchinson&#39;s Estimator is Bad at Kronecker-Trace-Estimation</a></h4> <p>We study the problem of estimating the trace of a matrix <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >A</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{ A}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68611em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol">A</span></span></span></span></span></span> that can only be access via Kronecker-matrix-vector products. That is, we can compute <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >A</mi><mi mathvariant=bold >x</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{ A}\mathbf{ x}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68611em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol">A</span></span></span><span class=mord ><span class="mord mathbf">x</span></span></span></span></span> for any vector <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold >x</mi></mrow><annotation encoding="application/x-tex">\mathbf{ x}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44444em;vertical-align:0em;"></span><span class=mord ><span class="mord mathbf">x</span></span></span></span></span> that has Kronecker structure. In particular, we study how Hutchinson&#39;s Estimator performs in this setting, proving tight rates for the number of matrix-vector products this estimator needs to find a relative error approximation to the trace of <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant=bold-italic >A</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{ A}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68611em;vertical-align:0em;"></span><span class=mord ><span class=mord ><span class="mord boldsymbol">A</span></span></span></span></span></span>. We find an exact expression for the variance of this estimator, show this is unavoidably exponential, and conclude with evidence that a much faster non-Hutchinson algorithm may exist.</p> <script> var acc = document.getElementsByClassName("theorem-accordion"); var i; for (i = 0; i < acc.length; i++) { acc[i].addEventListener("click", function() { this.classList.toggle("active"); var panel = this.nextElementSibling; if (panel.style.maxHeight) { panel.style.maxHeight = null; } else { panel.style.maxHeight = panel.scrollHeight + "px"; } }); } </script> <div class=page-foot > <div class=copyright > &copy; Raphael Arkady Meyer. Last modified: October 19, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> using the tufte theme and the <a href="https://julialang.org">Julia programming language</a>. <a href="/disclaimer">NYU Hosting Disclaimer.</a> </div> </div> </div> </div> </div>