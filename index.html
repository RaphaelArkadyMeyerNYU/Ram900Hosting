<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <link rel=stylesheet  href="/libs/katex/katex.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/tufte.css"> <link rel=stylesheet  href="/css/latex.css"> <link rel=stylesheet  href="/css/adjust.css"> <link rel=stylesheet  href="/css/theorem.css"> <link rel=stylesheet  href="/css/algorithm.css"> <link rel=icon  href="/assets/favicon.ico"> <title>Raphael A. Meyer</title> <div id=layout > <div id=menu > <ul> <li><a href="/">Home</a> <li><a href="/assets/raphael_a_meyer_cv.pdf">CV</a> <li><a href="/hutchplusplus" style="padding: 10px 8px">Hutch++</a> <li><a href="/siam-nnp-minisymposium-2023" style="padding: 10px 0px">SIAM-NNP</a> <li><a href="/thesis-defense" style="padding: 10px 0px">Thesis</a> <li><a href="https://randnla.github.io/" style="padding: 10px 0px">RandNLA Wiki</a> </ul> </div> <div id=main > <div class=franklin-content ><h1 id=raphael_arkady_meyer ><a href="#raphael_arkady_meyer" class=header-anchor >Raphael Arkady Meyer</a></h1> <div class=row ><div class=container ><div class=left ><img src="/assets/profile_2022.jpg" alt="" /></div></div> <p>I am a Postdoc at UC Berkeley, hosted by <a href="https://www.stat.berkeley.edu/~mmahoney/">Michael Mahoney</a> in the Stats Department and ICSI. Before that, I was a Postdoc at Caltech working with <a href="https://tropp.caltech.edu/">Joel Tropp</a>. Before that, I finished my Ph.D. at NYU advised by <a href="https://www.chrismusco.com">Christopher Musco</a>.</p> <p>I research problems in mathematical computing from the perspective of theoretical computer science.</p> <p>In the summer of 2022, I visited <a href="https://theory.epfl.ch/kapralov/">Michael Kapralov&#39;s</a> group at EPFL and <a href="http://www.math.tau.ac.il/~haimav/">Haim Avron&#39;s</a> group at TAU.</p></div> <p>Links: <a href="https://scholar.google.com/citations?user&#61;Xpi5HD0AAAAJ">Google Scholar</a>, <a href="https://dblp.org/pid/204/4381.html">dblp</a>, <a href="https://github.com/RaphaelArkadyMeyerNYU">Github</a>, <a href="https://nyu.zoom.us/my/ram900">Zoom Room</a></p> <p>My recent publications have looked at:</p> <ul> <li><p>Fast Numerical Linear Algebra &#40;<em><a href="https://arxiv.org/abs/2508.21189">preprint</a></em>, <em><a href="https://arxiv.org/abs/2311.14023">SIMAX</a></em>, <em><a href="https://arxiv.org/abs/2305.02535">SODA2024</a></em>&#41;</p> <li><p>Tensor-based Sketching Methods &#40;<em><a href="https://arxiv.org/abs/2508.21189">preprint</a></em>, <em><a href="https://arxiv.org/abs/2309.04952">preprint</a></em>, <em><a href="https://arxiv.org/abs/2502.08029">ICML2025</a></em>&#41;</p> <li><p>Active Learning on Linear Function Families &#40;<em><a href="https://arxiv.org/abs/2508.05920">preprint</a></em>,<em><a href="https://arxiv.org/abs/2211.06790">SODA2023</a></em>&#41;</p> </ul> <p>Of course, I am interested in problems beyond these areas, and if you want to work with me on a problem, send me an email: <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mi>a</mi><mi>m</mi><mn>900</mn><mi mathvariant=normal >@</mi><mi>b</mi><mi>e</mi><mi>r</mi><mi>k</mi><mi>e</mi><mi>l</mi><mi>e</mi><mi>y</mi><mi mathvariant=normal >.</mi><mi>e</mi><mi>d</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">ram900@berkeley.edu</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal">a</span><span class="mord mathnormal">m</span><span class=mord >9</span><span class=mord >0</span><span class=mord >0</span><span class=mord >@</span><span class="mord mathnormal">b</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class=mord >.</span><span class="mord mathnormal">e</span><span class="mord mathnormal">d</span><span class="mord mathnormal">u</span></span></span></span></p> <h1 id=news ><a href="#news" class=header-anchor >News</a></h1> <ul> <li><p><img style="width:25px;height:18px;padding-left:0", src="/assets/knuths-new.gif"> I&#39;m talking about Kronecker Matrix-Vector complexity this Tuesday at the Simon&#39;s Institute <a href="https://sites.google.com/cornell.edu/rpod-matrix-computations/about">Recent Progress and Open Directions in Matrix Computations</a> seminar.</p> <li><p><img style="width:25px;height:18px;padding-left:0", src="/assets/knuths-new.gif"> New paper on arXiv: <a href="https://arxiv.org/abs/2508.21189"><em>Faster Linear Algebra Algorithms with Structured Random Matrices</em></a>. Introducing the oblivious subspace <em>injection</em>&#33;</p> <li><p>I joined <a href="https://www.stat.berkeley.edu/~mmahoney/">Michael Mahoney</a>&#39;s group as a postdoc at UC Berkeley &#40;officially in the <a href="http://stat.berkeley.edu/">Statistics department</a> and at <a href="https://www.icsi.berkeley.edu/">ICSI</a>&#41;.</p> </ul> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mphantom><mi mathvariant=normal >.</mi></mphantom></mrow><annotation encoding="application/x-tex">\phantom{.}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.10556em;vertical-align:0em;"></span><span class=mord  style="color:transparent;">.</span></span></span></span> <button class=theorem-accordion ><div class=theorem-accordion-text > <em>Old News</em> </div></button><div class=theorem-panel ><p></p> <p>August 2025</p> <ul> <li><p>New paper on arXiv: <a href="https://arxiv.org/abs/2508.06486"><em>Does block size matter in randomized block Krylov low-rank approximation?</em></a>. Punchline: Block Krylov finds a rank-<span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span> approximation with <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy=false >(</mo><mi>k</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">O(k)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class=mopen >(</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class=mclose >)</span></span></span></span> matrix-vector products, regardless of the block size&#33;</p> <li><p>New paper on arXiv: <a href="https://arxiv.org/abs/2508.05920"><em>Debiasing Polynomial and Fourier Regression</em></a>. <a href="https://chriscamano.github.io/">Chris Camaño</a> wrote beautiful code for it, available on <a href="https://github.com/chriscamano/Debiased-Polynomial-Regression">github</a>&#33; Punchline: you can do DPP sampling for polynomial regression by computing the eigenvalues of random complex matrices&#33;</p> </ul> <p>July 2025</p> <ul> <li><p>I presented a poster for my paper accepted to ICML: <a href="https://arxiv.org/abs/2502.08029"><em>Understanding the Kronecker Matrix-Vector Complexity of Linear Algebra</em></a>.</p> </ul> <p>June 2025</p> <ul> <li><p>I gave a talk on Kronecker Matrix-Vector Complexity at Tom Trogdon&#39;s <a href="https://faculty.washington.edu/trogdon/RMT&#43;NLA_II/index.html">RMT &#43; NLA II</a> workshop this past June&#33;</p> </ul> <p>April 2025</p> <ul> <li><p>I gave a talk at the <a href="https://sites.google.com/ucsd.edu/ucsd-minds/home">UCSD MINDS Seminar</a> on April 18th – thanks to <a href="https://sites.google.com/ucsd.edu/rwebber/">Rob Webber</a> for inviting me out&#33;</p> </ul> <p>March 2025</p> <ul> <li><p>I recently gave a guest lecture in <a href="https://web.eecs.umich.edu/~girasole/">Laura Balzano&#39;s</a> Randomized Numerical Linear Algebra class at UMich&#33; It was great fun, and many great questions were asked. Thanks for having me&#33; You can find my slides <a href="/assets/uMichGuestLecture80minPPT.pdf">here</a>.</p> <li><p>I made a website to contain the cleanest known proofs for foundational concepts in RandNLA: <a href="https://randnla.github.io/">link</a>. <em>I actually made it a while ago and don&#39;t update it often.</em></p> <li><p>New preprint on arXiv: <a href="https://arxiv.org/abs/2502.08029"><em>Understanding the Kronecker Matrix-Vector Complexity of Linear Algebra</em></a>. Punchline: there&#39;s too much orthogonality&#33;</p> <li><p>I&#39;m talking about low-rank approximation &#40;<a href="https://arxiv.org/abs/2311.14023">this paper</a> and <a href="https://arxiv.org/abs/2305.02535">this paper</a>&#41; at Caltech on Friday Feb 14th. Come on out&#33;</p> </ul> <p>January 2025</p> <ul> <li><p>Updated version of the Kronecker Trace Estimation paper &#40;<a href="https://arxiv.org/abs/2309.04952">link</a>&#41;. New strengthened results on random rank-one matrices, Rademacher test vectors, and random unit vector test vectors&#33;</p> </ul> <p>October 2024</p> <ul> <li><p>I&#39;m at SIAM MDS giving a talk on recent and upcoming results on Kronecker Matrix-Vector Complexity. Come say hi&#33; Thanks to <a href="https://www.ethanepperly.com/">Ethan</a> and <a href="https://sites.google.com/ucsd.edu/rwebber/">Rob</a> for organizing the minisymposium&#33;</p> <li><p>Paper accepted at SIMAX: <a href="https://arxiv.org/abs/2311.14023"><em>Algorithm-Agnostic Low-Rank Approximation of Operator Monotone Matrix Functions</em></a>. My first paper in a math journal&#33;</p> </ul> <p>May 2024</p> <ul> <li><p>I&#39;ll be joining Caltech as a postdoc in <a href="https://tropp.caltech.edu/">Joel Tropp</a>&#39;s group this fall&#33;</p> <li><p>I&#39;ve been awarded the Pearl Brownstein Doctoral Research Award &#40;i.e. best dissertation award&#41; for my research&#33; Big thanks to the Tandon CSE department for awarding this to me, and congrats to the other awardees, <a href="https://aeciosantos.com/">Aecios</a> and <a href="https://www.mengweiren.com/">Mengwei</a>&#33;</p> <li><p>I gave a talk at the Center for Communications Research on <a href="https://arxiv.org/abs/2010.09649">Trace Estimation</a> and <a href="https://arxiv.org/abs/2309.04952">Kronecker-Trace Estimation</a>.</p> </ul> <p>April 2024</p> <ul> <li><p><strong>I just successfully defended my thesis, on April 16th 2024&#33;</strong> See details about my talk here: <a href="/thesis-defense">link</a>.</p> </ul> <p>March 2024</p> <ul> <li><p>The Responsible AI Lab at NYU Tandon &#40;who I have been a part of for the past two years&#41; wrote a spotlight on me&#33; Thanks to Caterina and the R/AI team for writing this up. See the short interview here: <a href="https://mailchi.mp/nyu/rai-march2024-newsletter?e&#61;82659192cb">link</a>.</p> </ul> <p>January 2024</p> <ul> <li><p>I presented my work on <a href="https://arxiv.org/abs/2010.09649">Trace Estimation</a> and <a href="https://arxiv.org/abs/2311.14023"><span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy=false >(</mo><mi>A</mi><mo stretchy=false >)</mo></mrow><annotation encoding="application/x-tex">f(A)</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class=mopen >(</span><span class="mord mathnormal">A</span><span class=mclose >)</span></span></span></span> Low-Rank Approximation</a> at the <a href="https://www.stat.berkeley.edu/~mmahoney/index.html">Mahoney Group</a> at UC Berkeley.</p> <li><p>I presented my work on <a href="https://arxiv.org/abs/2305.02535">Krylov methods</a> at SODA 2024.</p> </ul> <p>November 2023</p> <ul> <li><p>New preprint on arXiv: <a href="https://arxiv.org/abs/2311.14023"><em>Algorithm-Agnostic Low-Rank Approximation of Operator Monotone Matrix Functions</em></a>.</p> <li><p>I gave a talk on <a href="https://arxiv.org/abs/2305.02535">Krylov methods</a> at the <a href="https://www.math.purdue.edu/~xiaj/FastSolvers2023/index.html">Conference on Fast Direct Solvers</a> at Purdue University in November.</p> <li><p>I gave a <a href="https://orecchia.net/event/theory-lunch/">talk at UChicago</a> on <a href="https://arxiv.org/abs/2010.09649">Trace Estimation</a> and <a href="https://arxiv.org/abs/2309.04952">Kronecker-Trace Estimation</a> on November 1st.</p> </ul> <p>October 2023</p> <ul> <li><p>Paper accepted at SODA 2024: <em><a href="https://arxiv.org/abs/2305.02535">On the Unreasonable Effectiveness of Single Vector Krylov Methods for Low-Rank Approximation</a></em>&#33;</p> <li><p>I organized a minisymposium on <em>The Matrix-Vector Complexity of Linear Algebra</em> at the first ever <a href="https://sites.google.com/view/siam-nynjpa/annual-meeting">SIAM-NNP conference</a>&#33;</p> <p><a href="https://sites.google.com/view/shyamnarayanan/home">Shyam Narayanan</a>, <a href="https://e.math.cornell.edu/people/halikias/">Diana Halikias</a>, <a href="https://wswartworth.github.io/">William Swartworth</a>, <a href="https://chen.pw/">Tyler Chen</a>, and myself were presenting at 8:30am on Sunday. What a stacked lineup&#33; <strong>See the details here: <a href="/siam-nnp-minisymposium-2023">link</a>.</strong></p> </ul> <p>September 2023</p> <ul> <li><p>New preprint on arXiv: <a href="https://arxiv.org/abs/2309.04952"><em>Hutchinson’s Estimator is Bad at Kronecker-Trace-Estimation</em></a>.</p> </ul> <p>May 2023</p> <ul> <li><p>New preprint on arXiv: <a href="https://arxiv.org/abs/2305.02535"><em>On the Unreasonable Effectiveness of Single Vector Krylov Methods for Low-Rank Approximation</em></a>.</p> </ul> <p>March 2023</p> <ul> <li><p>I gave two talks at the NYU / UMass Quantum Linear Algebra reading group.</p> <li><p>I gave a talk at the BIRS Perspectives on Matrix Computations about my <a href="https://arxiv.org/abs/2305.02535"><em>new work on Krylov methods</em></a>.</p> </ul> <p>January 2023</p> <ul> <li><p>I presented <a href="https://arxiv.org/abs/2211.06790"><em>Near-Linear Sample Complexity for <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.969438em;vertical-align:-0.286108em;"></span><span class=mord ><span class="mord mathnormal">L</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> Polynomial Regression</em></a> at SODA 2023.</p> </ul> <p>November 2022</p> <ul> <li><p>I gave a talk at the <a href="https://theorypurdue.wordpress.com/">TCS Seminar at Purdue</a> in early November to present my new research on the role of block size in Krylov Methods.</p> </ul> <p>October 2022</p> <ul> <li><p>New paper accepted at SODA 2023: <em>Near-Linear Sample Complexity for <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.969438em;vertical-align:-0.286108em;"></span><span class=mord ><span class="mord mathnormal">L</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> Polynomial Regression</em>&#33; I just gave a talk on it last week Friday at the Grad Student Seminar at CDS &#40;at NYU&#41;.</p> </ul> <p>September 2022</p> <ul> <li><p>I gave a talk at <a href="https://sites.google.com/view/gammanla2022/home">GAMM ANLA</a> on the role of block size in Krylov Methods for low-rank approximation. A preprint will be available very soon, but until then you can check out my slides for a preview&#33; <a href="/assets/svkGammAnla20minBeamers.pdf">Slides</a></p> </ul> <p>July 2022</p> <ul> <li><p>I gave a talk at the <em>SIAM Annual Meeting Minisymposium on Matrix Functions, Operator Functions, and Related Approximation Methods</em>. Thanks to Heather, Andrew, and Ke for organizing&#33;</p> </ul> <p>June 2022</p> <ul> <li><p>I&#39;m going be presenting Hutch&#43;&#43; this summer at <a href="https://www.lse.ac.uk/HALG-2022">HALG2022</a>, with both a short talk and a poster.</p> <li><p>I&#39;m traveling this summer&#33; I&#39;m first in London for <a href="https://www.lse.ac.uk/HALG-2022">HALG2022</a>. Then I&#39;m spending June visiting <a href="http://www.math.tau.ac.il/~haimav/">Haim Avron</a> at <a href="https://english.tau.ac.il/">TAU</a>, and July visiting <a href="https://theory.epfl.ch/kapralov/">Michael Kapralov</a> at <a href="https://www.epfl.ch/en/">EPFL</a>. If you&#39;re in the same place at the same time, <a href="mailto:ram900@nyu.edu">drop me a line</a>&#33;</p> </ul> <p>May 2022</p> <ul> <li><p>I recently organized a mini-conference for NYU CS Theory researchers to present their &quot;Pandemic Papers&quot; in-person. Thanks to everyone who showed up and made it a success&#33; <a href="/tcs_presentations"><em>More details here</em></a></p> <li><p>I&#39;m honored to be awarded the <strong>Deborah Rosenthal, MD Award for Best Quals Examination</strong> in 2022, for my presentation <em>Towards Optimal Spectral Sum Estimation in the Matrix-Vector Oracle Model</em>.</p> </ul> <p>April 2022</p> <ul> <li><p>I&#39;m honored to be a ICLR 2022 Highlighted Reviewer.</p> </ul> <p></p></div> <h1 id=publications ><a href="#publications" class=header-anchor >Publications</a></h1> <div class=link-hover-only ><ol> <li><p><a href="https://arxiv.org/abs/2508.21189"><strong>Faster Linear Algebra Algorithms with Structured Random Matrices</strong></a></p> <span class=fakeclass > in submission <em>with <a href="https://chriscamano.github.io/">Chris Camaño</a>, <a href="https://www.ethanepperly.com/">Ethan N. Epperly</a>, and <a href="https://tropp.caltech.edu/">Joel A. Tropp</a></em></span> <li><p><a href="https://arxiv.org/abs/2508.06486"><strong>Does block size matter in randomized block Krylov low-rank approximation?</strong></a></p> <span class=fakeclass > in submission <em>with <a href="https://research.chen.pw/">Tyler Chen</a>, <a href="https://www.ethanepperly.com/">Ethan N. Epperly</a>, <a href="https://www.chrismusco.com/">Christopher Musco</a>, and <a href="https://www.linkedin.com/in/akashgrao/">Akash Rao</a></em></span> <li><p><a href="https://arxiv.org/abs/2508.05920"><strong>Debiasing Polynomial and Fourier Regression</strong></a><sup id="fnref:DebiasedPolyAssets"><a href="#fndef:DebiasedPolyAssets" class=fnref >[1]</a></sup></p> <span class=fakeclass > in submission <em>with <a href="https://chriscamano.github.io/">Chris Camaño</a> and <a href="https://kevinshu.me/">Kevin Shu</a></em></span> <li><p><a href="https://arxiv.org/abs/2502.08029"><strong>Understanding the Kronecker Matrix-Vector Complexity of Linear Algebra</strong></a><sup id="fnref:ICML2025assets"><a href="#fndef:ICML2025assets" class=fnref >[2]</a></sup></p> <span class=fakeclass > at ICML 2025 <em>with <a href="https://wswartworth.github.io/">William Swartworth</a> and <a href="http://www.cs.cmu.edu/~dwoodruf/">David P. Woodruff</a></em></span> <li><p><a href="https://arxiv.org/abs/2309.04952"><strong>Hutchinson&#39;s Estimator is Bad at Kronecker-Trace-Estimation</strong></a><sup id="fnref:KronHutchinsonAssets"><a href="#fndef:KronHutchinsonAssets" class=fnref >[3]</a></sup></p> <span class=fakeclass > in submission <em>with <a href="http://www.math.tau.ac.il/~haimav/">Haim Avron</a></em></span> <li><p><a href="https://arxiv.org/abs/2311.14023"><strong>Algorithm-Agnostic Low-Rank Approximation of Operator Monotone Matrix Functions</strong></a></p> <span class=fakeclass > in SIMAX <em>with <a href="https://scholar.google.com/citations?user&#61;jOtDnRAAAAAJ&amp;hl&#61;en&amp;oi&#61;ao">David Persson</a> and <a href="https://www.chrismusco.com/">Christopher Musco</a></em></span> <li><p><a href="https://arxiv.org/abs/2305.02535"><strong>On the Unreasonable Effectiveness of Single Vector Krylov Methods for Low-Rank Approximation</strong></a><sup id="fnref:SODA2024assets"><a href="#fndef:SODA2024assets" class=fnref >[4]</a></sup></p> <span class=fakeclass > at SODA 2024 <em>with <a href="https://people.cs.umass.edu/~cmusco/">Cameron Musco</a> and <a href="https://www.chrismusco.com/">Christopher Musco</a></em></span> <li><p><a href="https://arxiv.org/abs/2211.06790"><strong>Near-Linear Sample Complexity for <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.969438em;vertical-align:-0.286108em;"></span><span class=mord ><span class="mord mathnormal">L</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> Polynomial Regression</strong></a><sup id="fnref:SODA2023assets"><a href="#fndef:SODA2023assets" class=fnref >[5]</a></sup></p> <span class=fakeclass > at SODA 2023 <em>with <a href="https://people.cs.umass.edu/~cmusco/">Cameron Musco</a>, <a href="https://www.chrismusco.com/">Christopher Musco</a>, <a href="http://www.cs.cmu.edu/~dwoodruf/">David P. Woodruff</a>, and <a href="https://samsonzhou.github.io/">Samson Zhou</a></em></span> <li><p><a href="https://arxiv.org/abs/2203.07557"><strong>Fast Regression for Structured Inputs</strong></a><sup id="fnref:ICLR2022assets"><a href="#fndef:ICLR2022assets" class=fnref >[6]</a></sup></p> <span class=fakeclass > at ICLR 2022 <em>with <a href="https://people.cs.umass.edu/~cmusco/">Cameron Musco</a>, <a href="https://www.chrismusco.com/">Christopher Musco</a>, <a href="http://www.cs.cmu.edu/~dwoodruf/">David P. Woodruff</a>, and <a href="https://samsonzhou.github.io/">Samson Zhou</a></em></span> <li><p><a href="https://arxiv.org/abs/2010.09649"><strong>Hutch&#43;&#43;: Optimal Stochastic Trace Estimation</strong></a><sup id="fnref:hutchassets"><a href="#fndef:hutchassets" class=fnref >[7]</a></sup></p> <span class=fakeclass > at SOSA 2021 <em>with <a href="https://people.cs.umass.edu/~cmusco/">Cameron Musco</a>, <a href="https://www.chrismusco.com/">Christopher Musco</a>, and <a href="http://www.cs.cmu.edu/~dwoodruf/">David P. Woodruff</a></em></span> <li><p><a href="https://arxiv.org/abs/2006.08035"><strong>The Statistical Cost of Robust Kernel Hyperparameter Tuning</strong></a><sup id="fnref:NeurIPS2020assets"><a href="#fndef:NeurIPS2020assets" class=fnref >[8]</a></sup></p> <span class=fakeclass > at NeurIPS 2020 <em>with <a href="https://www.chrismusco.com/">Christopher Musco</a></em></span> <li><p><span class=link-hover-only > <a href="https://arxiv.org/abs/1901.09087"><strong>Optimality Implies Kernel Sum Classifiers are Statistically Efficient</strong></a></span><sup id="fnref:kernelsumassets"><a href="#fndef:kernelsumassets" class=fnref >[9]</a></sup></p> <span class=fakeclass > at ICML 2019 <em>with <a href="https://www.cs.purdue.edu/homes/jhonorio/">Jean Honorio</a></em></span> <li><p><span class=link-hover-only > <a href="https://www.cs.purdue.edu/homes/hmaji/papers/ISIT:JhaMajMey17.pdf"><strong>Characterizing Optimal Security and Round-Complexity for Secure OR Evaluation</strong></a></span></p> <span class=fakeclass > at ISIT 2017 <em>with Amisha Jhanji and <a href="https://www.cs.purdue.edu/homes/hmaji/">Hemanta K. Maji</a></em></span> </ol></div> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mphantom><mi mathvariant=normal >.</mi></mphantom></mrow><annotation encoding="application/x-tex">\phantom{.}</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.10556em;vertical-align:0em;"></span><span class=mord  style="color:transparent;">.</span></span></span></span> <button class=theorem-accordion ><div class=theorem-accordion-text > <em>Presentation Assets</em> </div></button><div class=theorem-panel ><p></p> <p><table class=fndef  id="fndef:DebiasedPolyAssets"> <tr> <td class=fndef-backref ><a href="#fnref:DebiasedPolyAssets">[1]</a> <td class=fndef-content >Code available on <a href="https://github.com/chriscamano/Debiased-Polynomial-Regression">github</a> </table> <table class=fndef  id="fndef:ICML2025assets"> <tr> <td class=fndef-backref ><a href="#fnref:ICML2025assets">[2]</a> <td class=fndef-content ><a href="/assets/ICML-2025-Poster.pdf">Poster</a> </table> <table class=fndef  id="fndef:KronHutchinsonAssets"> <tr> <td class=fndef-backref ><a href="#fnref:KronHutchinsonAssets">[3]</a> <td class=fndef-content ><a href="/assets/kronHutchinsonSiamNNP20min.pdf">Slides</a> </table> <table class=fndef  id="fndef:SODA2024assets"> <tr> <td class=fndef-backref ><a href="#fnref:SODA2024assets">[4]</a> <td class=fndef-content >Code available on <a href="https://github.com/RaphaelArkadyMeyerNYU/SingleVectorKrylov">github</a> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44445em;vertical-align:0em;"></span><span class=mord >⋅</span></span></span></span> <a href="/assets/svkSoda20minOneNote.pdf">Slides using TCS language</a> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44445em;vertical-align:0em;"></span><span class=mord >⋅</span></span></span></span> <a href="/assets/svkPurdue20minOneNote.pdf">Slides using Applied Math language</a> </table> <table class=fndef  id="fndef:SODA2023assets"> <tr> <td class=fndef-backref ><a href="#fnref:SODA2023assets">[5]</a> <td class=fndef-content ><a href="/assets/chebyPandemicPresentationsBeamers.pdf">Slides</a> </table> <table class=fndef  id="fndef:ICLR2022assets"> <tr> <td class=fndef-backref ><a href="#fnref:ICLR2022assets">[6]</a> <td class=fndef-content ><a href="/assets/ICLR-2022-Poster.png">Poster</a> </table> <table class=fndef  id="fndef:hutchassets"> <tr> <td class=fndef-backref ><a href="#fnref:hutchassets">[7]</a> <td class=fndef-content >Code available on <a href="https://github.com/RaphaelArkadyMeyerNYU/hutchplusplus">github</a> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44445em;vertical-align:0em;"></span><span class=mord >⋅</span></span></span></span> <a href="/assets/hutchplusplusposter.pdf">Landscape Poster</a> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44445em;vertical-align:0em;"></span><span class=mord >⋅</span></span></span></span> <a href="/assets/hutchplusplusHalgPoster.pdf">Portrait Poster</a> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44445em;vertical-align:0em;"></span><span class=mord >⋅</span></span></span></span> <a href="/assets/hutchplusplusHalg4minBeamers.pdf">4min Slides</a> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44445em;vertical-align:0em;"></span><span class=mord >⋅</span></span></span></span> <a href="/assets/hutchplusplusSosa12minBeamers.pdf">12min Slides</a> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44445em;vertical-align:0em;"></span><span class=mord >⋅</span></span></span></span> <a href="/assets/hutchplusplusSosa25minBeamers.pdf">25min Slides</a> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44445em;vertical-align:0em;"></span><span class=mord >⋅</span></span></span></span> <a href="/assets/hutchplusplusVida35minBeamers.pdf">35min Slides</a> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44445em;vertical-align:0em;"></span><span class=mord >⋅</span></span></span></span> <a href="/assets/hutchplusplusJHU1hrBeamers.pdf">1hr Slides</a> </table> <table class=fndef  id="fndef:NeurIPS2020assets"> <tr> <td class=fndef-backref ><a href="#fnref:NeurIPS2020assets">[8]</a> <td class=fndef-content ><a href="/assets/NeurIPS-2020-Beamers.pdf">Slides</a> </table> <table class=fndef  id="fndef:kernelsumassets"> <tr> <td class=fndef-backref ><a href="#fnref:kernelsumassets">[9]</a> <td class=fndef-content ><a href="/assets/ICML-2019-Poster.pdf">Poster</a> <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋅</mo></mrow><annotation encoding="application/x-tex">\cdot</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.44445em;vertical-align:0em;"></span><span class=mord >⋅</span></span></span></span> <a href="/assets/ICML-2019-slides.pdf">Slides</a>. </table> </p></div> <h1 id=talks_presentations ><a href="#talks_presentations" class=header-anchor >Talks &amp; Presentations</a></h1> <p>To date, I have presented every paper I published at the associated conference. This is a list of other talks or presentations I have given.</p> <button class=theorem-accordion ><div class=theorem-accordion-text > Invited Talks </div></button><div class=theorem-panel ><p></p> <div class=link-hover-only ><ol> <li><p><strong>Guest Lecture on Trace Estimation</strong><sup id="fnref:UMich2025assets"><a href="#fndef:UMich2025assets" class=fnref >[10]</a></sup></p> <span class=fakeclass > at <em><a href="https://web.eecs.umich.edu/~girasole/">Laura Balzano&#39;s</a> Randomized NLA course at the University of Michigan</em>.</span> <li><p><strong>On the Unreasonable Effectiveness of Single Vector Krylov for Low-Rank Approximation</strong></p> <span class=fakeclass > at <em>Caltech SIAM Student/Postoc Seminar</em>.</span> <li><p><strong>Kronecker Matrix-Vector Complexity is Strange</strong><sup id="fnref:SiamMDS2025assets"><a href="#fndef:SiamMDS2025assets" class=fnref >[11]</a></sup></p> <span class=fakeclass > at <em>Randomized Matrix Computations for Large-scale Scientific and Machine Learning Problems</em>, a minisymposium at SIAM Conference on Mathematics of Data Science &#40;MDS24&#41;</span> <li><p><strong>Optimal Trace Estimation and Sub-Optimal Kronecker-Trace Estimation</strong></p> <span class=fakeclass > at <em>Center for Communications Research</em>.</span> <li><p><strong>Optimal Trace Estimation and Algorithm-Agnostic funNystrom Guarantees</strong></p> <span class=fakeclass > at <a href="https://www.stat.berkeley.edu/~mmahoney/index.html"><em>UC Berkeley Mahoney Group Meeting</em></a>.</span> <li><p><strong>Optimal Trace Estimation and Sub-Optimal Kronecker-Trace Estimation</strong></p> <span class=fakeclass > at <a href="https://orecchia.net/event/theory-lunch/"><em>U Chicago Theory Lunch</em></a>.</span> <li><p><strong>On the Unreasonable Effectiveness of Single Vector Krylov for Low-Rank Approximation</strong></p> <span class=fakeclass > at <a href="https://www.birs.ca/events/2023/5-day-workshops/23w5108"><em>BIRS workshop on Perspectives on Matrix Computations</em></a>.</span> <li><p><strong>On the Unreasonable Effectiveness of Single Vector Krylov for Low-Rank Approximation</strong></p> <span class=fakeclass > at <em><a href="https://theorypurdue.wordpress.com/">Purdue University TCS Seminar</a></em></span> <li><p><strong>Near-Linear Sample Complexity for <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.969438em;vertical-align:-0.286108em;"></span><span class=mord ><span class="mord mathnormal">L</span><span class=msupsub ><span class="vlist-t vlist-t2"><span class=vlist-r ><span class=vlist  style="height:0.15139200000000003em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class=pstrut  style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class=vlist-s >​</span></span><span class=vlist-r ><span class=vlist  style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span> Polynomial Regression</strong></p> <span class=fakeclass > at <em>NYU CDS Student Seminar</em></span> <li><p><strong>Hutch&#43;&#43; and More: Towards Optimal Spectral Sum Estimation</strong></p> <span class=fakeclass > at <em>Matrix Functions, Operator Functions, and Related Approximation Methods</em>, a minisymposium at SIAM Annual Meeting &#40;AN22&#41;</span> <li><p><strong>Hutch&#43;&#43;: Optimal Stochastic Trace Estimation</strong></p> <span class=fakeclass > at <em>John Hopkins University Theory Seminar</em></span> <li><p><strong>Lessons from Trace Estimation Lower Bounds: Testing, Communication, and Anti-Concentration</strong><sup id="fnref:SiamAN2021assets"><a href="#fndef:SiamAN2021assets" class=fnref >[12]</a></sup></p> <span class=fakeclass > at <em>Computational Lower Bounds in Numerical Linear Algebra</em>, a minisymposium at SIAM Annual Meeting &#40;AN21&#41;</span> </ol></div> <p><table class=fndef  id="fndef:UMich2025assets"> <tr> <td class=fndef-backref ><a href="#fnref:UMich2025assets">[10]</a> <td class=fndef-content >Slides available <a href="/assets/uMichGuestLecture80minPPT.pdf">here</a> </table> <table class=fndef  id="fndef:SiamMDS2025assets"> <tr> <td class=fndef-backref ><a href="#fnref:SiamMDS2025assets">[11]</a> <td class=fndef-content >Slides available <a href="/assets/SiamMds24KronMatVec20minPPT.pdf">here</a> </table> <table class=fndef  id="fndef:SiamAN2021assets"> <tr> <td class=fndef-backref ><a href="#fnref:SiamAN2021assets">[12]</a> <td class=fndef-content >Slides available <a href="/assets/SiamAN-2021-Beamers.pdf">here</a>. Video starts at 1:04:55 <a href="https://player.vimeo.com/video/578316017#t&#61;1h4m55s">here</a>. </table> </p></div> <button class=theorem-accordion ><div class=theorem-accordion-text > Other Conference Presentations </div></button><div class=theorem-panel ><p></p> <div class=link-hover-only ><ol> <li><p><strong>Kronecker Matrix-Vector Complexity is Strange</strong></p> <span class=fakeclass > Short Talk at the <a href="https://faculty.washington.edu/trogdon/RMT&#43;NLA_II/">RMN &#43; NLA II Conference</a></span> <li><p><strong>On the Unreasonable Effectiveness of Single Vector Krylov for Low-Rank Approximation</strong><sup id="fnref:PresentationSectionAssets"><a href="#fndef:PresentationSectionAssets" class=fnref >[13]</a></sup></p> <span class=fakeclass > Short Talk at <a href="https://www.math.purdue.edu/~xiaj/FastSolvers2023/index.html">Conference on Fast Direct Solvers</a></span> <li><p><strong>Hutchinson&#39;s Estimator is Bad at Kronecker-Trace-Estimation</strong><sup id="fnref:PresentationSectionAssets"><a href="#fndef:PresentationSectionAssets" class=fnref >[13]</a></sup></p> <span class=fakeclass > Short Talk at <a href="/siam-nnp-minisymposium-2023"><em>SIAM-NNP 2022</em></a></span> <li><p><strong>On the Unreasonable Effectiveness of Single Vector Krylov for Low-Rank Approximation</strong><sup id="fnref:PresentationSectionAssets"><a href="#fndef:PresentationSectionAssets" class=fnref >[13]</a></sup></p> <span class=fakeclass > Short Talk at <a href="https://sites.google.com/view/gammanla2022/home"><em>GAMM ANLA 2022</em></a>.</span> <li><p><strong>Hutch&#43;&#43;: Optimal Stochastic Trace Estimation</strong><sup id="fnref:PresentationSectionAssets"><a href="#fndef:PresentationSectionAssets" class=fnref >[13]</a></sup></p> <span class=fakeclass > Poster and Short Talk at <a href="https://www.lse.ac.uk/HALG-2022"><em>HALG 2022</em></a>.</span> <li><p><strong>Chebyshev Sampling is Optimal for Lp Polynomial Regression</strong><sup id="fnref:PresentationSectionAssets"><a href="#fndef:PresentationSectionAssets" class=fnref >[13]</a></sup></p> <span class=fakeclass > Talk at <a href="/tcs_presentations"><em>NYU &quot;Pandemic Presentations&quot; 2022</em></a></span> <li><p><strong>Hutch&#43;&#43;: Optimal Stochastic Trace Estimation</strong><sup id="fnref:PresentationSectionAssets"><a href="#fndef:PresentationSectionAssets" class=fnref >[13]</a></sup></p> <span class=fakeclass > Poster at <a href="https://waldo2021.github.io/"><em>Wald&#40;O&#41; 2021</em></a>.</span> <li><p><strong>Optimality Implies Kernel Sum Classifiers are Statistically Efficient</strong><sup id="fnref:PresentationSectionAssets"><a href="#fndef:PresentationSectionAssets" class=fnref >[13]</a></sup></p> <span class=fakeclass > Poster at <em>Midwest Theory Day 2019</em></span> </ol></div> <p><table class=fndef  id="fndef:PresentationSectionAssets"> <tr> <td class=fndef-backref ><a href="#fnref:PresentationSectionAssets">[13]</a> <td class=fndef-content >Assets available in the <a href="#publications">Publications</a> section. </table> </p></div> <button class=theorem-accordion ><div class=theorem-accordion-text > Talks at Reading Groups </div></button><div class=theorem-panel ><p> <em>This list is no longer updated. The existing slides are useful though, so I&#39;m keeping this around.</em></p> <div class=link-hover-only ><ol> <li><p><strong>Feature Importance Impossibility Theorems</strong><sup id="fnref:StoyanovichGroup2024"><a href="#fndef:StoyanovichGroup2024" class=fnref >[14]</a></sup></p> <span class=fakeclass > 45-min-long talk at NYU RAI Reading Group</span> <li><p><strong>Fairwashing SHAP &#40;aka Interventional and Observational Shapley Values&#41;</strong><sup id="fnref:StoyanovichGroup2023"><a href="#fndef:StoyanovichGroup2023" class=fnref >[15]</a></sup></p> <span class=fakeclass > 45-min-long talk at NYU RAI Reading Group</span> <li><p><strong>The Equivalence of Matrix-Vector Complexity in Quantum Computing, Part 2</strong></p> <span class=fakeclass > 1-hour-long talk at NYU/UMass Quantum Linear Algebra Reading Group</span> <li><p><strong>The Equivalence of Matrix-Vector Complexity in Quantum Computing, Part 1</strong></p> <span class=fakeclass > 1-hour-long talk at NYU/UMass Quantum Linear Algebra Reading Group</span> <li><p><strong>Hutch&#43;&#43;: Optimal Stochastic Trace Estimation</strong></p> <span class=fakeclass > 1-hour-long talk at NYU VIDA RG Reading Group</span> <li><p><strong>Introduction to Leverage Scores</strong></p> <span class=fakeclass > 1.5-hour-long talk at NYU Tandon Theory Reading Group</span> <li><p><strong>Strategies for Episodic Tabular &amp; Linear MDPs</strong></p> <span class=fakeclass > Two 1.5-hour-long talks at NYU Tandon Reinforcement Learning Reading Group</span> <li><p><strong>Lagrangian Duality</strong></p> <span class=fakeclass > Three 1.5-hour-long talks at NYU Tandon Theory Reading Group</span> <li><p><strong>Introduction to Differential Entropy</strong></p> <span class=fakeclass > 1-hour-long talk at NYU CDS Reading Group on Information Theory</span> <li><p><strong>Lower bounds on the complexity of stochastic convex optimization</strong><sup id="fnref:MuscoGroup2019"><a href="#fndef:MuscoGroup2019" class=fnref >[16]</a></sup></p> <span class=fakeclass > 1-hour-long presentation of the paper <em>Information-Theoretic Lower Bounds on the Oracle Complexity of Stochastic Convex Optimization</em> by Agarwal et. al.</span> </ol></div> <p><table class=fndef  id="fndef:StoyanovichGroup2024"> <tr> <td class=fndef-backref ><a href="#fnref:StoyanovichGroup2024">[14]</a> <td class=fndef-content >Link to relevant paper <a href="https://arxiv.org/pdf/2212.11870.pdf">here</a>. </table> <table class=fndef  id="fndef:StoyanovichGroup2023"> <tr> <td class=fndef-backref ><a href="#fnref:StoyanovichGroup2023">[15]</a> <td class=fndef-content >Link to relevant paper <a href="https://arxiv.org/pdf/2006.16234.pdf">here</a>. My slides available <a href="/assets/StoyanovichGroup-2023-Beamers.pdf">here</a>. </table> <table class=fndef  id="fndef:MuscoGroup2019"> <tr> <td class=fndef-backref ><a href="#fnref:MuscoGroup2019">[16]</a> <td class=fndef-content >Link to the original paper <a href="https://arxiv.org/abs/1009.0571">here</a>. My slides available <a href="/assets/MuscoGroup-2019-Beamers.pdf">here</a>. </table> </p></div> <h1 id=teaching ><a href="#teaching" class=header-anchor >Teaching</a></h1> <p>I really enjoy teaching, and have been a TA for a few courses now:</p> <ul> <li><p><a href="https://dataresponsibly.github.io/rds24/">Responsible Data Science</a>, New York University, Spring 2024</p> <li><p><a href="https://www.chrismusco.com/amlds2023/">Algorithmic Machine Learning and Data Science</a>, New York University, Fall 2023</p> <li><p><a href="https://dataresponsibly.github.io/rds23/">Responsible Data Science</a>, New York University, Spring 2023</p> <li><p><a href="https://www.chrismusco.com/amlds2020/">Algorithmic Machine Learning and Data Science</a>, New York University, Fall 2020</p> <li><p><a href="https://www.chrismusco.com/introml2020/">Introduction to Machine Learning</a>, New York University, Spring 2020</p> <li><p><a href="https://www.cs.purdue.edu/homes/pdrineas/documents/CS381-Fall18/index.html">Introduction to the Analysis of Algorithms</a>, Purdue University, Fall 2018</p> </ul> <h1 id=service ><a href="#service" class=header-anchor >Service</a></h1> <p>Service outside of reviewing:</p> <ol> <li><p>Organizer for the <a href="/siam-nnp-minisymposium-2023">Minisymposium &quot;The Matrix-Vector Complexity of Linear Algebra&quot; at SIAM-NNP 2023</a></p> <li><p>Organizer for <a href="/tcs_presentations">NYU TCS &quot;Pandemic Presentations&quot; Day</a></p> <li><p>Organizer for NYU Tandon Theory Reading Group</p> </ol> <p>Service as a reviewer:</p> <ol> <li><p>ICML 2024 Reviewer</p> <li><p>NeurIPS 2024 Reviewer</p> <li><p>FOCS 2024 External Reviewer</p> <li><p>IMA Journal of Numerical Analysis 2024 Reviewer</p> <li><p>ICALP 2024 External Reviewer</p> <li><p>ICML 2024 Reviewer</p> <li><p>IJCAI 2024 Reviewer</p> <li><p>ICLR 2024 Reviewer</p> <li><p>NeurIPS 2023 Reviewer</p> <li><p>TMLR 2023 Reviewer</p> <li><p>ICLR 2023 Reviewer</p> <li><p>SODA 2023 External Reviewer</p> <li><p>NeurIPS 2022 Reviewer</p> <li><p>ICML 2022 Reviewer</p> <li><p>STOC 2022 External Reviewer</p> <li><p>ICLR 2022 Reviewer*</p> <li><p>NeurIPS 2021 Reviewer*</p> <li><p>ISIT 2017 External Reviewer</p> </ol> <p><em>* Denotes Highlighted / Outstanding Reviewer</em></p> <script> var acc = document.getElementsByClassName("theorem-accordion"); var i; for (i = 0; i < acc.length; i++) { acc[i].addEventListener("click", function() { this.classList.toggle("active"); var panel = this.nextElementSibling; if (panel.style.maxHeight) { panel.style.maxHeight = null; } else { panel.style.maxHeight = panel.scrollHeight + "px"; } }); } </script> <div class=page-foot > <div class=copyright > &copy; Raphael Arkady Meyer. Last modified: September 08, 2025. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> using the tufte theme and the <a href="https://julialang.org">Julia programming language</a>. <a href="/disclaimer">NYU Hosting Disclaimer.</a> </div> </div> </div> </div> </div>