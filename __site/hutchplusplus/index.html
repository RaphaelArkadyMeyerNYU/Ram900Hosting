<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
     <link rel="stylesheet" href="/libs/katex/katex.min.css">
     
     <link rel="stylesheet" href="/libs/highlight/github.min.css">
   
    <link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/tufte.css">
<link rel="stylesheet" href="/css/latex.css">
<link rel="stylesheet" href="/css/adjust.css"> <!-- sheet to overwrite some clashing styles -->
<link rel="stylesheet" href="/css/theorem.css">
<link rel="stylesheet" href="/css/algorithm.css">
<link rel="icon" href="/assets/favicon.ico">

     <title>Hutch++ | Raphael A. Meyer</title>  
</head>

<body>
<div id="layout">
  <div id="menu">
    <ul>
      <li><a href="/">Home</a></li>
      <li><a href="/assets/raphael_a_meyer_cv.pdf">CV</a></li>
      <!-- <li><a href="/hutchplusplus" style="padding: 10px 8px">Hutch++</a></li> -->
    </ul>
  </div>
  <div id="main">



<!-- Content appended here -->
<div class="franklin-content"><p>   </p>
<h1 id="updates_for_hutch"><a href="#updates_for_hutch">Updates for Hutch&#43;&#43;</a></h1>
<span style="color:red; font-size: 30px">Work in Progress. This page is not complete.</span>
<p>At SOSA 2021, I presented the paper <a href="https://arxiv.org/pdf/2010.09649.pdf">Hutch&#43;&#43;: Optimal Stochastic Trace Estimation</a> &#40;with Cameron Musco, Christopher Musco, and David P. Woodruff&#41;. This webpage contains some short updates that may be useful to people.</p>
<p>Table of Contents:</p>
<p><div class="franklin-toc"><ol><li><a href="#updates_for_hutch">Updates for Hutch&#43;&#43;</a></li><li><a href="#hutch_for_undergrads">Hutch&#43;&#43; for Undergrads</a><ol><li><a href="#part_1_the_fundamental_shape_of_hutchinsons_estimator">Part 1: The Fundamental Shape of Hutchinson&#39;s Estimator</a></li><li><a href="#part_2_hutchinsons_versus_the_top_few_eigenvalues">Part 2: Hutchinson&#39;s versus the Top Few Eigenvalues</a></li><li><a href="#part_3_the_variance_of_hutch">Part 3: The Variance of Hutch&#43;&#43;</a></li><li><a href="#part_4_practitioner_advice">Part 4: Practitioner Advice</a></li></ol></li><li><a href="#nystroumlm-hutch">Nystr&ouml;m-Hutch&#43;&#43;</a></li><li><a href="#epilogue">Epilogue</a><ol><li><a href="#code">Code</a></li><li><a href="#quiz_questions">Quiz Questions</a></li><li><a href="#footnotes">Footnotes</a></li><li><a href="#references">References</a></li></ol></li></ol></div> </p>
<h1 id="hutch_for_undergrads"><a href="#hutch_for_undergrads">Hutch&#43;&#43; for Undergrads</a></h1>
<p>I believe that the analysis of both Hutchinson&#39;s and Hutch&#43;&#43; is simple and approachable. This section contains the analysis an advanced undergraduate should hopefully be able to understand.</p>
<h2 id="part_1_the_fundamental_shape_of_hutchinsons_estimator"><a href="#part_1_the_fundamental_shape_of_hutchinsons_estimator">Part 1: The Fundamental Shape of Hutchinson&#39;s Estimator</a></h2>
 <div class="theorem">
			<h2 class="theorem-header" id="Definition 1">
				<a href="#Definition 1"> Definition 1:  Hutchinson&#39;s Estimator</a>
				<div class="theorem-type"> Definition</div>
			</h2>
			<div class="theorem-content"> 	Fix \(\boldsymbol{ A} \in \mathbb R^{n \times n}\) be a real matrix, and fix \(\ell\in\mathbb N\). 	Let \(\mathbf{ x}_1,\ldots,\mathbf{ x}_\ell \in\mathbb R^n\) be vectors drawn with i.i.d. \(\mathcal N(0,1)\) entries. 	Then, Hutchinson&#39;s Estimator is</p>
\[
		H_\ell(\boldsymbol{ A}) \;{\vcentcolon=}\; \frac1\ell \sum_{i=1}^\ell \mathbf{ x}_i^\intercal \boldsymbol{ A} \mathbf{ x}_i
	\]
<p></div></div>
<p>We can easily verify two properties of this classical estimator for symmetric \(\boldsymbol{ A}\):</p>
 <div class="theorem">
			<h2 class="theorem-header" id="Lemma 1">
				<a href="#Lemma 1"> Lemma 1:  Hutchinson&#39;s Mean and Variance</a>
				<div class="theorem-type"> Lemma</div>
			</h2>
			<div class="theorem-content"> \(\mathbb E[H_\ell(\boldsymbol{ A})] = \text{tr}(\boldsymbol{ A})\) and \(\text{Var}[H_\ell(\boldsymbol{ A})]=2\| \boldsymbol{ A}\|_{ F}^2\) </div></div>
<button class="theorem-accordion"><div class="theorem-accordion-text"> 	 <em>Proof</em> </div></button><div class="theorem-panel"><p> <div class="proof-box"><p><em><strong>Proof.</strong></em> The expectation is easy to verify:</p>
\[
	\mathbb E[H_\ell(\boldsymbol{ A})] = \mathbb E[\mathbf{ x}^\intercal\boldsymbol{ A}\mathbf{ x}] = \sum_{i=1}^n \sum_{j=1}^n \mathbb E[[\boldsymbol{ A}]_{i,j}\mathbf{ x}_i\mathbf{ x}_j] = \sum_{i=1}^n \boldsymbol{ A}_{i,i} = \text{tr}(\boldsymbol{ A})
\]
<p>In order to analyze the variance, we take the analysis of Lemma 9 from <span class="bibref"><a href="#avron2011randomized">Avron, Toledo (2011)</a></span>. Let \(\boldsymbol{ A}=\boldsymbol{ U}\boldsymbol{ \Lambda}\boldsymbol{ U}^\intercal\) be the eigendecomposition of \(\boldsymbol{ A}\), and let \(\mathbf{ y} \;{\vcentcolon=}\; \boldsymbol{ U}^\intercal\mathbf{ x}\). Then, for \(\mathbf{ x}\sim\mathcal N(0,\boldsymbol{ I})\), we have \(\mathbf{ y} \sim\mathcal N(0,\boldsymbol{ U}\boldsymbol{ U}^\intercal) = \mathcal N(0,\boldsymbol{ I})\). Therefore,</p>
\[\begin{aligned}
	\text{Var}[H_\ell(\boldsymbol{ A})]
	&= \frac1\ell \text{Var}[\mathbf{ x}^\intercal\boldsymbol{ A}\mathbf{ x}] \\
	&= \frac1\ell \text{Var}[\mathbf{ x}^\intercal\boldsymbol{ U}\boldsymbol{ \Lambda}\boldsymbol{ U}^\intercal\mathbf{ x}] \\
	&= \frac1\ell \text{Var}[(\boldsymbol{ U}^\intercal\mathbf{ x})^\intercal\boldsymbol{ \Lambda}(\boldsymbol{ U}\mathbf{ x})] \\
	&= \frac1\ell \text{Var}[\mathbf{ y}^\intercal\boldsymbol{ \Lambda}\mathbf{ y}] \\
	&= \frac1\ell \text{Var}[\sum_{i=1}^n \lambda_i y_i^2] \\
	&= \frac1\ell \sum_{i=1}^n \text{Var}[\lambda_i y_i^2] \\
	&= \frac1\ell \sum_{i=1}^n 2\lambda_i^2 \\
	&= \frac2\ell \| \boldsymbol{ A}\|_{ F}^2
\end{aligned}\]
<p>Note we cannot just use linearity of trace naively on the sum \(\mathbf{ x}^\intercal\boldsymbol{ A}\mathbf{ x}=\sum_{i=1}^n\sum_{j=1}^n x_ix_j \boldsymbol{ A}_{i,j}\) since linearity of variance only holds for independent samples. <div style="text-align:right"> \(\blacksquare \, \, \) </div></p></div> </p></div>
<p>This variance is in term of the <em>Frobenius</em> norm of \(\boldsymbol{ A}\). So, we expect that Hutchinson&#39;s Estimator satisfies \(H_\ell(\boldsymbol{ A}) \in \text{tr}(\boldsymbol{ A}) \pm \frac{\sqrt2}{\sqrt\ell} \| \boldsymbol{ A}\|_{ F}\) with high probability.<sup id="fnref:highprobability"><a href="#fndef:highprobability" class="fnref">[1]</a></sup> Typically in computer science we are given an error tolerance \(\varepsilon\), and want to know how many matrix-vector products we need to compute. &#40;i.e. how small can we make \(\ell\)?&#41; In order to build a guarantee here, we <em>assume \(\boldsymbol{ A}\) is positive semi-definite &#40;PSD&#41;</em>, which in turn implies that \(\| \boldsymbol{ A}\|_{ F} \leq \text{tr}(\boldsymbol{ A})\). With this in mind, we can prove the standard result for Hutchinson&#39;s Estimator:</p>
 <div class="theorem">
			<h2 class="theorem-header" id="Lemma 2">
				<a href="#Lemma 2"> Lemma 2:  Hutchinson&#39;s Estimator</a>
				<div class="theorem-type"> Lemma</div>
			</h2>
			<div class="theorem-content"> 	Fix \(\boldsymbol{ A} \in \mathbb R^{d \times d}\) be a PSD matrix. 	Then, \(\text{Var}[H_\ell(\boldsymbol{ A})]\leq\frac{2}{\ell}\text{tr}^2(\boldsymbol{ A})\) </div></div>
<p>So, if we want to ensure \(H_\ell(\boldsymbol{ A})\in(1\pm\varepsilon)\text{tr}(\boldsymbol{ A})\) with good probability, we need \(k=O(\frac{1}{\varepsilon^2})\) samples.</p>
<h2 id="part_2_hutchinsons_versus_the_top_few_eigenvalues"><a href="#part_2_hutchinsons_versus_the_top_few_eigenvalues">Part 2: Hutchinson&#39;s versus the Top Few Eigenvalues</a></h2>
<p><em>This was a section in the first draft of the paper, but was unfortunately cut. This is a slow, methodical intuition for the geometry Hutch&#43;&#43; takes advantage of.</em></p>
 <div class="theorem">
			<h2 class="theorem-header" id="Theorem 1">
				<a href="#Theorem 1"> Theorem 1:  Hutchinson&#39;s Estimator</a>
				<div class="theorem-type"> Theorem</div>
			</h2>
			<div class="theorem-content"> The Counter Reads </div></div>
<h2 id="part_3_the_variance_of_hutch"><a href="#part_3_the_variance_of_hutch">Part 3: The Variance of Hutch&#43;&#43;</a></h2>
<p><em>Special thanks to <a href="http://users.cms.caltech.edu/~jtropp/">Joel A. Tropp</a> for working most of this out, and encouraging us to look into sharpening the constants in the analysis of Hutch&#43;&#43;.</em></p>
<p>By expressing the variance of Hutch&#43;&#43; with all of its constants laid bare, and by using a very simple analysis, this analysis will hopefully allow practitioners to easily the exact parameters in Hutch&#43;&#43; code.</p>
<p>Before bounding the variance of Hutch&#43;&#43;, we include the proof of one lemma and import another theorem:</p>
 <div class="theorem">
			<h2 class="theorem-header" id="l2-l1-l0">
				<a href="#l2-l1-l0"> Lemma 3:  L2/L1/L0 Bounds</a>
				<div class="theorem-type"> Lemma</div>
			</h2>
			<div class="theorem-content"> Let \(\boldsymbol{ A}\in\mathbb R^{d \times d}\) be a PSD matrix. Let \(\boldsymbol{ A}_k\) be the best rank-k approximation to \(\boldsymbol{ A}\). Then, \(\| \boldsymbol{ A}-\boldsymbol{ A}_k\|_{ F} \leq \frac{1}{2\sqrt k} \text{tr}(\boldsymbol{ A})\). </div></div>
<button class="theorem-accordion"><div class="theorem-accordion-text"> 	 <em>Proof: Lemma 7 from <span class="bibref"><a href="#gilbert2007one">Gilbert et al. (2007)</a></span></em> </div></button><div class="theorem-panel"><p> <div class="proof-box"><p><em><strong>Proof.</strong></em> Let \(\mathbf{ v}=[\lambda_1 \ldots \lambda_n]\) be the vector of eigenvalues of \(\boldsymbol{ A}\), such that \(\lambda_1 \geq \lambda_2 \geq ... \ \lambda_n \geq 0\). Then \(\mathbf{ v}_k \;{\vcentcolon=}\; [\lambda_1 \ldots \lambda_k \ 0 \ldots 0]\) is the vector of eigenvalues of \(\boldsymbol{ A}_k\). With this notation, we just need to prove that \(\| \mathbf{ v}-\mathbf{ v}_k\|_{ 2} \leq \frac{1}{2\sqrt k} \| \mathbf{ v}\|_{ 1}\). The rest of this proof is just Lemma 7 from <a href="https://www.math.uci.edu/~rvershyn/papers/one-sketch-forall-complete.pdf">this paper</a>.</p>
<p>H&ouml;lder&#39;s inequality states that \(\sum_{i=1}^n a_i b_i \leq \| \mathbf{ a}\|_{ p} \cdot \| \mathbf{ b}\|_{ q}\) for any vector \(\mathbf{ a},\mathbf{ b}\) so long as \(\frac1p + \frac1q = 1\). If we take \(\mathbf{ a}=\mathbf{ b}=\mathbf{ v}-\mathbf{ v}_k\), \(p=1\), and \(q=1\), we find that \(\| \mathbf{ v}-\mathbf{ v}_k\|_{ 2}^2 \leq \| \mathbf{ v}-\mathbf{ v}_k\|_{ 1} \ \| \mathbf{ v}-\mathbf{ v}_k\|_{ \infty}\). Further, note that by our construction of \(\mathbf{ v}_k\), \(\| \mathbf{ v}\|_{ 1} = \| \mathbf{ v}_k\|_{ 1} + \| \mathbf{ v}-\mathbf{ v}_k\|_{ 1}\). Then,</p>
\[
	\frac{\| \mathbf{ v}-\mathbf{ v}_k\|_{ 2}}{\| \mathbf{ v}\|_{ 1}}
	\leq
	\frac{\sqrt{\| \mathbf{ v}-\mathbf{ v}_k\|_{ 1} \ \| \mathbf{ v}-\mathbf{ v}_k\|_{ \infty}}}{\| \mathbf{ v}_k\|_{ 1} + \| \mathbf{ v}-\mathbf{ v}_k\|_{ 1}}
\]
<p>Note that \(\| \mathbf{ v}-\mathbf{ v}_k\|_{ \infty} = \lambda_{k+1}\) and \(\| \mathbf{ v}_k\|_{ 1} = \sum_{i=1}^k  \lambda_i \geq k\lambda_{k+1}\). Then, if we let \(\gamma \;{\vcentcolon=}\; \| \mathbf{ v}-\mathbf{ v}_k\|_{ 1}\), we find</p>
\[
	\frac{\sqrt{\| \mathbf{ v}-\mathbf{ v}_k\|_{ 1} \ \| \mathbf{ v}-\mathbf{ v}_k\|_{ \infty}}}{\| \mathbf{ v}_k\|_{ 1} + \| \mathbf{ v}-\mathbf{ v}_k\|_{ 1}}
	\leq
	\frac{\sqrt{\gamma \ \lambda_{k+1}}}{k\lambda_{k+1} + \gamma}
\]
<p>We don&#39;t have a good way to bound \(\gamma = \| \mathbf{ v}-\mathbf{ v}_k\|_{ 1}\), so we instead notice that the function \(\gamma \mapsto \frac{\sqrt{a\gamma}}{b+\gamma}\) is maximized at \(\gamma=b\) &#40;check with calculus&#41;. So, \(\frac{\sqrt{a\gamma}}{b+\gamma} \leq \frac{\sqrt{ab}}{2b}\) for all \(\gamma\):</p>
\[
	\frac{\sqrt{\gamma \ \lambda_{k+1}}}{k\lambda_{k+1} + \gamma}
	\leq
	\frac{\sqrt{(k\lambda_{k+1}) \lambda_{k+1}}}{2k\lambda_{k+1}}
	= \frac{1}{2\sqrt k}
\]
<p>So, overall, we have shown that</p>
\[
	\frac{\| \boldsymbol{ A}-\boldsymbol{ A}_k\|_{ F}}{\text{tr}(\boldsymbol{ A})} = \frac{\| \mathbf{ v}-\mathbf{ v}_k\|_{ 2}}{\| \mathbf{ v}\|_{ 1}} \leq \frac{1}{2\sqrt k}
\]
<div style="text-align:right"> \(\blacksquare \, \, \) </div></div> </p></div>
<p>We also import the following theorem, which controls the expected error from our low-rank approximation, and which we do not prove:</p>
 <div class="theorem">
			<h2 class="theorem-header" id="tropp-error">
				<a href="#tropp-error"> Theorem 2:  Expected Projection Error</a>
				<div class="theorem-type"> Theorem</div>
			</h2>
			<div class="theorem-content"> Sample \(\boldsymbol{ S}\in\mathbb R^{n \times (k+p)}\) with \(\mathcal N(0,1)\) entries. Let \(\boldsymbol{ Q}\) be any orthonormal basis for \(\boldsymbol{ A}\boldsymbol{ S}\) &#40;e.g. a QR decomposition&#41;. Then,</p>
\[
	\mathbb E[\| (\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A}\|_{ F}^2]
	\leq
	(1+{\textstyle\frac{ k}{ p-1}})\| \boldsymbol{ A}-\boldsymbol{ A}_k\|_{ F}^2
\]
<p></div></div>
<button class="theorem-accordion"><div class="theorem-accordion-text"> 	 <em>Proof: Adapted from <span class="bibref"><a href="#halko2011finding">Halko et al. (2011)</a></span></em> </div></button><div class="theorem-panel"><p> <div class="proof-box"><p><em><strong>Proof.</strong></em> This is almost exactly the statement of Theorem 10.5 in <span class="bibref">(<a href="#halko2011finding">Halko et al. (2011)</a>)</span>, but not exactly. Theorem 10.5 bounds the expected Frobenius norm &#40;not squared&#41;. Instead, at the bottom of page 57 and top of page 58 from <a href="https://arxiv.org/pdf/0909.4061.pdf">this version of the paper</a>, we see that the proof of Theorem 10.5 actually proves</p>
\[
	\left(\mathbb E [\| (\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A}\|_{ F}^2]\right)^{1/2} 
	\leq \ldots
	\leq (1+\frac{k}{p-1})\| \boldsymbol{ \Sigma}_2\|_{ F}^2
\]
<p>Where, \(\| \boldsymbol{ \Sigma}_2\|_{ F}^2=\| \boldsymbol{ A}-\boldsymbol{ A}_k\|_{ F}^2\), completing the importing of this theorem. <div style="text-align:right"> \(\blacksquare \, \, \) </div></p></div> </p></div>
<p>Now, we proceed to the analysis of the variance of Hutch&#43;&#43;:</p>
<a id="thm_hutchpp_variance" class="anchor"></a>
 <div class="theorem">
			<h2 class="theorem-header" id="thm-hutchpp-variance">
				<a href="#thm-hutchpp-variance"> Theorem 3:  Variance of Hutch&#43;&#43;</a>
				<div class="theorem-type"> Theorem</div>
			</h2>
			<div class="theorem-content"> Fix parameters \(k\) and \(\ell\). Let \(p=2k+1\) and construct \(\boldsymbol{ Q}\in\mathbb R^{n \times k+p}\) as in <span class="bibref"><a href="#tropp-error">Theorem 2</a></span>. Then let</p>
\[
	\text{Hutch++}(\boldsymbol{ A}) \;{\vcentcolon=}\; \text{tr}(\boldsymbol{ Q}^\intercal\boldsymbol{ A}\boldsymbol{ Q}) + H_\ell((\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A})
\]
<p>Then,</p>
\[
	\mathbb E[\text{Hutch++}(\boldsymbol{ A})] = \text{tr}(\boldsymbol{ A})
\]
\[
	\text{Var}[\text{Hutch++}(\boldsymbol{ A})] \leq \frac{1}{k\ell}\text{tr}^2(\boldsymbol{ A})
\]
<p></div></div>
<button class="theorem-accordion"><div class="theorem-accordion-text"> 	 <em>Proof</em> </div></button><div class="theorem-panel"><p> <div class="proof-box"><p><em><strong>Proof.</strong></em></p>
<p>We first look at the expectation by using the Tower Law, the expectation of Hutchinson&#39;s estimator, the cyclic property of trace, and the linearity of trace: \[\begin{aligned}
	\mathbb E[\text{Hutch++}(\boldsymbol{ A})]
	&= \mathbb E[\text{tr}(\boldsymbol{ Q}^\intercal\boldsymbol{ A}\boldsymbol{ Q}) + H_\ell((\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A})] \\
	&= \mathbb E[\text{tr}(\boldsymbol{ Q}^\intercal\boldsymbol{ A}\boldsymbol{ Q})] + \mathbb E\left[\mathbb E[H_\ell((\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A})|\boldsymbol{ Q}]\right] \\
	&= \mathbb E[\text{tr}(\boldsymbol{ Q}^\intercal\boldsymbol{ A}\boldsymbol{ Q})] + \mathbb E[\text{tr}((\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A})] \\
	&= \mathbb E[\text{tr}(\boldsymbol{ A}\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)] + \mathbb E[\text{tr}(\boldsymbol{ A}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal\boldsymbol{ A})] \\
	&= \mathbb E[\text{tr}(\boldsymbol{ A}\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)+\text{tr}(\boldsymbol{ A}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal\boldsymbol{ A})] \\
	&= \text{tr}(\boldsymbol{ A})
\end{aligned}\] We now move onto the Variance bound. First, recall the <a href="https://en.wikipedia.org/wiki/Law_of_total_variance">Conditional Variance Formula</a>, which says for any random variables \(X,Y\),</p>
\[
	\text{Var}[Y] = \mathbb E[\text{Var}[Y|X]] + \text{Var}[\mathbb E[Y|X]]
\]
<p>Taking \(Y=\text{Hutch++}(\boldsymbol{ A})\) and \(X=\boldsymbol{ Q}\), we can bound the variance of Hutch&#43;&#43;:</p>
\[
	\text{Var}[\text{Hutch++}]
	= \mathbb E[\text{Var}[\text{Hutch++} | \boldsymbol{ Q}]] + \text{Var}[\mathbb E[\text{Hutch++} | \boldsymbol{ Q}]]
\]
<p>The second term above is always zero: \[\begin{aligned}
	\text{Var}[\mathbb E[\text{Hutch++}(\boldsymbol{ A}) | \boldsymbol{ Q}]]
	&= \text{Var}\left[\mathbb E[\text{tr}(\boldsymbol{ Q}^\intercal\boldsymbol{ A}\boldsymbol{ Q}) + H_\ell( (\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A}) | \boldsymbol{ Q}]\right] \\
	&= \text{Var}\left[\text{tr}(\boldsymbol{ Q}^\intercal\boldsymbol{ A}\boldsymbol{ Q}) + \text{tr}((\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A}) \right] \\
	&= \text{Var}\left[\text{tr}(\boldsymbol{ A})\right] \\
	&= 0
\end{aligned}\] So, we just need to bound the first term from the conditional variance formula: \[\begin{aligned}
	\mathbb E[\text{Var}[\text{Hutch++} | \boldsymbol{ Q}]]
	&= \mathbb E[\text{Var}[\text{tr}(\boldsymbol{ Q}^\intercal\boldsymbol{ A}\boldsymbol{ Q})+H_\ell( (\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A})|Q]] \\
	&= \mathbb E[\text{Var}[H_\ell( (\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A})|Q]] \\
	&\leq {\textstyle\frac{ 2}{ \ell}} \mathbb E[\| (\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A}\|_{ F}^2] \\
	&\leq {\textstyle\frac{ 2}{ \ell}} (1+\frac{k}{p-1})\| \boldsymbol{ A}-\boldsymbol{ A}_k\|_{ F}^2 \\
\end{aligned}\] Where the first inequality uses the variance of Hutchinson&#39;s Estimator, and the second uses <span class="bibref"><a href="#tropp-error">Theorem 2</a></span>. Recall that we set \(p=k+1\). After substituting and simplifying the expression, we find</p>
\[
	\mathbb E[\text{Var}[\text{Hutch++} | \boldsymbol{ Q}]]
	\leq {\textstyle\frac{ 4}{ \ell}}\| \boldsymbol{ A}-\boldsymbol{ A}_k\|_{ F}^2
\]
<p>And by <span class="bibref"><a href="#l2-l1-l0">Lemma 3</a></span>, we complete the proof:</p>
\[
	{\textstyle\frac{ 4}{ \ell}}\| \boldsymbol{ A}-\boldsymbol{ A}_k\|_{ F}^2 \leq {\textstyle\frac{ 4}{ \ell}} \cdot {\textstyle\frac{ 1}{ 4\ell}}\text{tr}^2(\boldsymbol{ A}) = {\textstyle\frac{ 1}{ k\ell}}\text{tr}^2(\boldsymbol{ A})
\]
<div style="text-align:right"> \(\blacksquare \, \, \) </div></div> </p></div>
<h2 id="part_4_practitioner_advice"><a href="#part_4_practitioner_advice">Part 4: Practitioner Advice</a></h2>
<p>The variance in <span class="bibref"><a href="#thm-hutchpp-variance">Theorem 3</a></span> still has two parameters in it, which leave a bit of ambiguity for practitioners. So, we quickly analyze the choice of \(k\) and \(\ell\) that minimizes our bound on the variance.</p>
<p>Formally, suppose we are allowed to compute exactly \(m\) matrix-vector products with \(\boldsymbol{ A}\). Then,</p>
<ul>
<li><p>Computing \(\boldsymbol{ Q}\) uses \(k+p=2k+1\) products</p>
</li>
<li><p>Computing \(\text{tr}(\boldsymbol{ Q}^\intercal\boldsymbol{ A}\boldsymbol{ Q})\) uses \(k+p=2k+1\) products</p>
</li>
<li><p>Computing \(H_\ell( (\boldsymbol{ I}-\boldsymbol{ Q}\boldsymbol{ Q}^\intercal)\boldsymbol{ A})\) uses \(\ell\) products</p>
</li>
</ul>
<p>So, we have \(m=2(2k+1)+\ell=4k+2+\ell\). We can then verify that \(k=\frac{m-2}{8}\) and \(\ell=\frac{m}{2}-1\) minimizes the variance. Notably, this is equivalent to setting \(\ell=4k\), which looks more intuitive. This produces a final variance of \(\text{Var}[\text{Hutch++}(\boldsymbol{ A})]\leq\frac{16}{(m-2)^2}\text{tr}^2(\boldsymbol{ A})\).</p>
<h1 id="nystroumlm-hutch"><a href="#nystroumlm-hutch">Nystr&ouml;m-Hutch&#43;&#43;</a></h1>
<p>One additional approach to implementing Hutch&#43;&#43; for PSD matrices considers the Nystr&ouml;m method, where we use the approximation</p>
\[
	\tilde\boldsymbol{ A}_k = (\boldsymbol{ A}\boldsymbol{ Q})(\boldsymbol{ Q}^\intercal\boldsymbol{ A}\boldsymbol{ Q})^{-1}(\boldsymbol{ A}\boldsymbol{ Q})^{\intercal}
\]
<p>Note that we can compute \(\tilde\boldsymbol{ A}_k=\boldsymbol{ Y}(\boldsymbol{ Q}^\intercal\boldsymbol{ Y})^{-1}\boldsymbol{ Y}^\intercal\) where \(\boldsymbol{ Y}=\boldsymbol{ A}\boldsymbol{ Q}\) contains all the matrix-vector products with \(\boldsymbol{ A}\). We can compute \(\text{tr}(\tilde\boldsymbol{ A}_k)\) efficiently:</p>
\[\begin{aligned}
	\text{tr}(\tilde\boldsymbol{ A}_k)
	&= \text{tr}(\boldsymbol{ Y}(\boldsymbol{ Q}^\intercal\boldsymbol{ Y})^{-1}\boldsymbol{ Y}^\intercal) \\
	&= \text{tr}((\boldsymbol{ Q}^\intercal\boldsymbol{ Y})^{-1}(\boldsymbol{ Y}^\intercal\boldsymbol{ Y}))
\end{aligned}\]
<p>The matrix inversion is only for a \((k+p) \times (k+p)\) sized matrix, which is easy. We can then define the <em><strong>NYS-Hutch&#43;&#43;</strong></em> estimator as</p>
\[\begin{aligned}
	\text{NYS-Hutch++}(\boldsymbol{ A})
	&\;{\vcentcolon=}\; \text{tr}((\boldsymbol{ Q}^\intercal\boldsymbol{ Y})^{-1}(\boldsymbol{ Y}^\intercal\boldsymbol{ Y})) + H_\ell(\boldsymbol{ A}-\tilde\boldsymbol{ A}_k) \\
\end{aligned}\]
<p>We can write out this Hutchinson&#39;s Estimator step as</p>
\[
	H_\ell(\boldsymbol{ A}-\tilde\boldsymbol{ A}_k) =
	\frac{1}{\ell}\left(\text{tr}(\boldsymbol{ G}^\intercal\boldsymbol{ A}\boldsymbol{ G}) - \text{tr}(\boldsymbol{ R}^\intercal(\boldsymbol{ Q}^\intercal\boldsymbol{ Y})^{-1}\boldsymbol{ R}) \right)
\]
<p>where \(\boldsymbol{ G}\) has random sign-bit entries and \(\boldsymbol{ R}\;{\vcentcolon=}\;\boldsymbol{ Y}^\intercal\boldsymbol{ G}\). Formally, we can write this out as the following algorithm:</p>
<div class="pseudocode"><p> <span style="font-size: 20px">Algorithm 1:  NYS-Hutch&#43;&#43;</span></p>
<p>---</p>
<p><strong>input</strong>: Matrix-Vector Oracle access to \(\boldsymbol{ A}\in\mathbb R^{d \times d}\). Number \(m\) of queries.</p>
<p><strong>output</strong>: Approximation to \(\text{tr}(\boldsymbol{ A})\)</p>
<ol>
<li><p>Sample \(\boldsymbol{ S}\in\mathbb R^{d \times \frac{m}{4}}\) and \(\boldsymbol{ G}\in\mathbb R^{d \times \frac{m}{2}}\) with i.i.d. \(\{+1,-1\}\) entries.</p>
</li>
<li><p>Compute an orthonormal basis \(\boldsymbol{ Q}\in\mathbb R^{d \times \frac{m}{4}}\) for the span of \(\boldsymbol{ A}\boldsymbol{ S}\)</p>
</li>
<li><p>Compute \(\boldsymbol{ Y}=\boldsymbol{ A}\boldsymbol{ Q}\) and \(\boldsymbol{ R}=\boldsymbol{ Y}^\intercal\boldsymbol{ G}\)</p>
</li>
<li><p>return \(\text{tr}((\boldsymbol{ Q}^\intercal\boldsymbol{ Y})^{-1}(\boldsymbol{ Y}^\intercal\boldsymbol{ Y})) + \frac{2}{m}\left(\text{tr}(\boldsymbol{ G}^\intercal\boldsymbol{ A}\boldsymbol{ G}) - \text{tr}(\boldsymbol{ R}^\intercal(\boldsymbol{ Q}^\intercal\boldsymbol{ Y})^{-1}\boldsymbol{ R}) \right)\)</p>
</li>
</ol></div>
<p>Or, in matlab:</p>
<pre><code class="language-julia">function trace_est&#61;simple_nystrom_hutchplusplus&#40;A, num_queries&#41;
    S &#61; 2*randi&#40;2,size&#40;A,1&#41;,ceil&#40;num_queries/4&#41;&#41;-3;
    G &#61; 2*randi&#40;2,size&#40;A,1&#41;,floor&#40;num_queries/2&#41;&#41;-3;
    &#91;Q,~&#93; &#61; qr&#40;A*S,0&#41;;
    Y &#61; A*Q;
    R &#61; Y&#39; * G;
    QYinv &#61; pinv&#40;Q&#39; * Y&#41;;
    trace_est &#61; trace&#40;QYinv * &#40;Y&#39;*Y&#41;&#41; &#43; 1/size&#40;G,2&#41;*&#91;trace&#40;G&#39;*A*G&#41; - trace&#40;R&#39;*QYinv*R&#41;&#93;;
end</code></pre>
<h1 id="epilogue"><a href="#epilogue">Epilogue</a></h1>
<h2 id="code"><a href="#code">Code</a></h2>
<ul>
<li><p>We provide <a href="https://github.com/RaphaelArkadyMeyerNYU/hutchplusplus">code on Github</a>, written in MATLAB.</p>
</li>
<li><p><a href="https://www.akshayagrawal.com/">Akshay Agrawal</a> also has an <a href="https://github.com/akshayka/hessian_trace_estimation">implementation on Github</a> using pytorch, though we haven&#39;t verified this code ourselves.</p>
</li>
<li><p>Email me if you want to add your implementation to the list&#33;</p>
</li>
</ul>
<h2 id="quiz_questions"><a href="#quiz_questions">Quiz Questions</a></h2>
<p>If you have read Hutch&#43;&#43;, then quiz yourself by trying to answer some intuitive high-level questions:</p>
<p>1.	Why is the constant-factor approximation on our low-rank approximation \(\|A-\tilde{A}_k\|_F \leq 2 \|A-A_k\|_F\) sufficient? Why don&#39;t we need a relative error approximation like \(\|A-\tilde{A}_k\|_F \leq (1+\varepsilon) \|A-A_k\|_F\)?</p>
<div style="text-align:right"> <em>Give an intuitive answer, not a mathematical one.</em> </div>
<h2 id="footnotes"><a href="#footnotes">Footnotes</a></h2>
<p><sup id="fnref:highprobability"><a href="#fndef:highprobability" class="fnref">[1]</a></sup> We analyze variance in this page for convenience, but all of these confidence intervals hold with high probability &#40;i.e. with logarithmic dependence on the failure probability&#41;, and this is widely analyzed in the formal publications.</p>
<h2 id="references"><a href="#references">References</a></h2>
<ul>
<li><p><a id="gilbert2007one" class="anchor"></a><strong>Gilbert</strong>, <strong>Strauss</strong>, <strong>Tropp</strong>, and <strong>Vershynin</strong>. <a href="https://www.math.uci.edu/~rvershyn/papers/one-sketch-forall-complete.pdf">One sketch for all: fast algorithms for compressed sensing</a>. <em>STOC</em> 2007.</p>
</li>
<li><p><a id="halko2011finding" class="anchor"></a><strong>Halko</strong>, <strong>Martinsson</strong>, and <strong>Tropp</strong>. <a href="https://arxiv.org/pdf/0909.4061.pdf">Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions</a>. <em>SIAM review 53 no. 2</em> 2011.</p>
</li>
<li><p><a id="avron2011randomized" class="anchor"></a><strong>Avron</strong> and <strong>Toledo</strong>. <a href="http://www.cs.tau.ac.il/~stoledo/Bib/Pubs/trace3.pdf">Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix</a>. <em>Journal of the ACM &#40;JACM&#41; 58, no. 2</em> 2011.</p>
</li>
</ul>

<script>
var acc = document.getElementsByClassName("theorem-accordion");
var i;

for (i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.maxHeight) {
      panel.style.maxHeight = null;
    } else {
      panel.style.maxHeight = panel.scrollHeight + "px";
    }
  });
}
</script>

<div class="page-foot">
  <div class="copyright">
    &copy; Raphael Arkady Meyer. Last modified: February 06, 2021. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> using the tufte theme and the <a href="https://julialang.org">Julia programming language</a>.
    <a href="/disclaimer">NYU Hosting Disclaimer.</a>
  </div>
</div>
</div><!-- CONTENT ENDS HERE -->
        </div> <!-- end of id=main -->
    </div> <!-- end of id=layout -->
    
        <script src="/libs/katex/katex.min.js"></script>
<script src="/libs/katex/auto-render.min.js"></script>
<script>renderMathInElement(document.body)</script>

    
    
        <script src="/libs/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
