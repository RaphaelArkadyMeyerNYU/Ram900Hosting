@def title = "Raphael A. Meyer"
@def tags = ["nyu", "student", "research"]

# Raphael Arkady Meyer


@@row
@@container
@@left ![](/assets/sm_profile.jpg) @@
@@
I am a $2^{nd}$ year Ph.D. Student at NYU Tandon School of Engineering, advised by [Prof. Christopher Musco](https://www.chrismusco.com) and part of the [Algorithms and Foundations Group](https://csefoundations.engineering.nyu.edu/).

<!-- To compile this for publishing, run `serve(single=true, clear=true, is_final_pass=true)` and upload the _site directory. Be sure to also run `using Franklin` -->


I research the interplay of theoretical statistics and computation, largely through the lens of linear algebra.

Links:
[Google Scholar](https://scholar.google.com/citations?user=Xpi5HD0AAAAJ), 
[dblp](https://dblp.org/pid/204/4381.html), 
[Github](https://github.com/RaphaelArkadyMeyerNYU), 
[Zoom Room](https://nyu.zoom.us/my/ram900), 
[NYU Tandon](https://engineering.nyu.edu/raphael-arkady-meyer)

~~~
<div style="clear: both"></div>
~~~
@@

My recent publications have looked at:

* Provably *Optimal* Fast Linear-Algebra Algorithms (_[SOSA2021](https://arxiv.org/pdf/2010.09649.pdf)_)
* The Statistical Complexity of Kernel Learning (_[NeurIPS2020](https://arxiv.org/pdf/2006.08035.pdf)_, _[ICML2019](https://arxiv.org/pdf/1901.09087.pdf)_)

Of course, I am interested in problems beyond these areas, and if you want to work with me on a problem, send me an email: $ram900@nyu.edu$

<!-- Some high-level open questions that I think are super interesting.

* When does *adaptivity* speed up fast linear algebra? Why doesn't adaptivity speed up trace estimation (i.e. sum of eigenvalues), but does seem to speed up estimating the top eigenvalue?
 -->

<!-- \tableofcontents --> <!-- you can use \toc as well -->

# Publications

1.  **Hutch++: Optimal Stochastic Trace Estimation**[^hutchcode]
    
    with [Cameron Musco](https://people.cs.umass.edu/~cmusco/), [Christopher Musco](https://www.chrismusco.com/), [David P. Woodruff](http://www.cs.cmu.edu/~dwoodruf/)
    
    at _Symposium on Simplicity in Algorithms (SOSA 2021)_

1.  **The Statistical Cost of Robust Kernel Hyperparameter Turning**
    
    with [Christopher Musco](https://www.chrismusco.com/)

    at _Advances in Neural Information Processing Systems (NeurIPS 2020)_

1.  **Optimality Implies Kernel Sum Classifiers are Statistically Efficient**

    with [Jean Honorio](https://www.cs.purdue.edu/homes/jhonorio/)

    at _International Conference on Machine Learning (ICML 2019)_
    
1.  **Characterizing Optimal Security and Round-Complexity for Secure OR Evaluation**

    with Amisha Jhanji, [Hemanta K. Maji](https://www.cs.purdue.edu/homes/hmaji/)

    at _IEEE International Symposium on Information Theory (ISIT 2017)_

[^hutchcode]: Code available on [github](https://github.com/RaphaelArkadyMeyerNYU/hutchplusplus)

# Teaching

I really enjoy teaching, and have been a TA for a few courses now:

* [Algorithmic Machine Learning and Data Science](https://www.chrismusco.com/amlds2020/), New York University, Fall 2020

* [Introduction to Machine Learning](https://www.chrismusco.com/introml/), New York University, Spring 2020

* [Introduction to the Analysis of Algorithms](https://www.cs.purdue.edu/homes/pdrineas/documents/CS381-Fall18/index.html), Purdue University, Fall 2018
